<html>
<head>
  <meta http-equiv="content-type" content ="text/html" charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Jia Chen</title>
  <link href="css/bootstrap.min.css" rel="stylesheet" media="screen">
  <link href="css/style.css" rel="stylesheet">
  <link rel="stylesheet" href="css/font-awesome-4.6.3/css/font-awesome.min.css">
  <link href="http://fonts.googleapis.com/css?family=Roboto:400,300,500" rel="stylesheet">
</head>
<body>
  <div class="container col-md-8 col-md-offset-2">
    <div class="page-header media">
      <div class="col-md-4 col-xs-4">
        <h1 class="title">Jia Chen (陈佳)</h1>
      </div>
      <div class="col-md-8 col-xs-14">
        <h1><small class="subtitle">
          Post Doctoral Associate<br>
          School of Computer Science<br>
          Carnegie Mellon University
        </small></h1>
      </div>
    </div>
    <nav>
      <ul id="menu" class="list-unstyled pull-right">
        <li><a href="index.html">Home</a></li>
        <li><a href="research.html" class="menu-active">Research</a></li>
        <li><a href="publications.html">Publications</a></li>
      </ul>
    </nav>
  </div>

  <div class="container col-md-8 col-md-offset-2">
    <h3 class="subheader">Research Projects</h3>
    <ul class="list-unstyled">

      <li class="research-item">
      <h4><img src="images/punt_gran.jpg">
        <span class="redcolor">I</span>mage/<span class="redcolor">V</span>ideo <span class="redcolor">D</span>escription with <span class="redcolor">N</span>atural <span class="redcolor">L</span>anguage
      </br><img src="images/line_p3.jpg" width="100%"></h4>
      <div class="media">
        <div class="media-left">
          <!-- <img class="media-object" src="images/framework.png" width="220px">
          <font size="1"> <center> Fig.1 The TGM Framework.</center></font> -->
          <img class="media-object" src="images/model_cases.png" width="220px">
          <!-- <font size="1"> <center>Fig.2 The Cases of the Models.</center></font> -->
        </div>
        <div class="media-body">
          <p>
          Generating natural language descriptions of visual content is an intriguing task. It has a wide range of applications such as text summarization for video preview, assisting blind people, or improving search quality for online videos. Our works focus on the following four main challenges for video captioning:  1) Multi-modalities, 2) Temporal movements, 3) Diverse topics 4)Wisdom of all
          </p> 
          <p>
          <span class="label label-success"><i class="fa fa-trophy"></i></span>
              Best Performer on Video Caption task at <a href="https://www-nlpir.nist.gov/projects/tv2018/Tasks/vtt/">TRECVID 2018</a>
          <br>
          <span class="label label-success"><i class="fa fa-trophy"></i></span>
              Best Performer on Video Caption task at <a href="https://www-nlpir.nist.gov/projects/tv2017/Tasks/vtt/">TRECVID 2017</a>
          <br>
          <span class="label label-success"><i class="fa fa-trophy"></i></span>
              Best Performer at <a href="http://ms-multimedia-challenge.com/2017/challenge">MSR Video to Language Challenge 2017</a>
          <br>
          <span class="label label-success"><i class="fa fa-trophy"></i></span> Best Grand Challenge Paper Award at ACM Multimedia 2017
          <br>
          <span class="label label-success"><i class="fa fa-trophy"></i></span>
              Best Performer at <a href="http://ms-multimedia-challenge.com/2016/challenge">MSR Video to Language Challenge 2016</a>
          <br>
          <span class="label label-warning"><i class="fa fa-code" aria-hidden="true"></i></span><a href="https://github.com/marshimarocj/caption_competition">Source Code</a>
          </p>
        </div>
      </div>
    </li>

    <li class="research-item">
    <h4>
      <span class="redcolor">D</span>eep <span class="redcolor">I</span>ntermodal <span class="redcolor">V</span>ideo <span class="redcolor">A</span>nalysis
      <br><img src="images/line_p3.jpg" width="100%">
    </h4>
    <div class="media">
      <div class="media-left">
        <img class="media-object" src="images/diva_demo.png" width="220px">
        <!-- <img class="media-object" src="images/diva_overview.png" width="220px"> -->
      </div>
      <div class="media-body">
        <p>
        DIVA addresses activity detection for both forensic applications and for real-time alerting. It aims to develop robust automated activity detection for a multi-camera streaming video environment. As an essential aspect of DIVA, activities will be enriched by person and object detection, as well as recognition at multiple levels of granularity. 
        <!-- <ul>
          <li>Detection of primitive activities occurring in ground-based video collection. Examples include Person getting into a vehicle, Person getting out of vehicle, Person carrying object</li>
          <li>Detection of complex activities, including pre-specified or newly defined activities. Examples include Person being picked up by vehicle, Person abandoning object, Two people exchanging an object</li>
        </ul> -->
        </p>
        <p>
          <span class="label label-success"><i class="fa fa-trophy"></i></span>Best performer at <a href="https://actev.nist.gov/prizechallenge#tab_leaderboard">activity detection challenge 2019</a> in surveillance videos hosted by NIST & IARPA. 
          <br>
          <span class="label label-success"><i class="fa fa-trophy"></i></span>2nd place at Surveillance Event Detection (SED) in <a href="https://www-nlpir.nist.gov/projects/tv2017/Tasks/sed/">TRECVID 2017</a>
          <br>
          <span class="label label-warning"><i class="fa fa-code" aria-hidden="true"></i></span>
          <a href="https://github.com/marshimarocj/conv_rnn_trn">Source Code (event classifer)</a>
          <br>
          <span class="label label-warning"><i class="fa fa-code" aria-hidden="true"></i></span>
          <a href="https://github.com/marshimarocj/sed2017">Source Code (SED2017)</a>
          <br>
          <span class="label label-primary"><i class="fa fa-bookmark-o"></i></span>&nbsp;
          <a href="https://www-nlpir.nist.gov/projects/tvpubs/tv18.papers/inf.pdf">Technical Report</a><!-- :  We develop an event detection system which shares many similarities with the standard object detection pipeline. It is composed of four modules: feature extraction, event proposal, event classification and event localization.  -->
      </div>
    </div>
    </li>

    <li class="research-item">
    <h4>
      <span class="redcolor">E</span>vent <span class="redcolor">R</span>econstruction
      <br><img src="images/line_p3.jpg" width="100%">
    </h4>
    <div class="media">
      <div class="media-left">
        <!-- <img class="media-object" src="images/boston_illustration.jpg" width="220px">
        <font size="1"> <center> Fig.1 Metaphor for Event Reconstruction. </center></font> -->
        <img class="media-object" src="images/boston_event.jpg" width="220px">
        <!-- <font size="1"> <center> Fig.2 "Boston Marathon 2013" Event. </center></font> -->
      </div>
      <div class="media-body">
        <p>
        When an event happens, different videos capture different moments of the same event at different positions from different perspectives. The situation is very similar to the story of the "blind men and an elephant". The event truth corresponds to the elephant in the story, and each single video corresponds to one of the blind men who only touches parts of the elephant, either in the time dimension or in the space dimension. The goal of event reconstruction is to recover the elephant in its entirety from each of the blind mens' descriptions.
        </p>
        <p>
          <span class="label label-info"><i class="fa fa-cube" aria-hidden="true"></i></span>
          <a href="http://aladdin1.inf.cs.cmu.edu:8081/boston/">Dataset</a><!-- : cover three major time periods of the event: the pre-explosion, the two explosions, and the post-explosion. They also cover different geographical areas of the event, including the point of the first explosion, the point of the second explosion, and the point where the evacuation teams gathered, and so on. -->
          <br>
          <span class="label label-warning"><i class="fa fa-code"></i></span>
          <a href="https://github.com/marshimarocj/gsv_crawler">Source Code</a><!-- : Given the center GPS coordinate and radius of a circle area, it crawls the Google Street View data, including panorama image, depth map and the transformation panoramas.  -->
          <br>
          <span class="label label-primary"><i class="fa fa-bookmark-o"></i></span>
          <a href="http://aladdin1.inf.cs.cmu.edu:8081/humanRightsData/public/cmu-lti-018.pdf">Technical Report</a><!-- : we introduce the first real-world event reconstruction dataset to promote research in this field. We focus on synchronization and localization, which are the two basic and essential elements for other tasks in event reconstruction such as person tracking, scene reconstruction, and object retrieval.
 -->        </p>
      </div>
    </div>
    </li>

    <!-- <li class="research-item">
      <h4><img src="images/punt_gran.jpg">
        <span class="redcolor">S</span>emantic <span class="redcolor">I</span>mage <span class="redcolor">P</span>rofiling for <span class="redcolor">H</span>istoric <span class="redcolor">E</span>vents
      </br><img src="images/line_p3.jpg" width="100%"></h4>
      <div class="media">
        <div class="media-left">
          <img class="media-object" src="images/history3.png" width="250px">
        </div>
        <div class="media-body">
          <p>
            History event related knowledge is precious and multimedia such as imagery is a powerful medium that records diverse information about the event. In this work, we automatically construct an image/multimedia profile given a one sentence description of the historic event which contains where, when, who and what elements. Such a simple input requirement makes our solution easy to scale up and support a wide range of culture preservation and curation related applications ranging from wikipedia enrichment to history education. Furthermore, we automatically add explicit semantic information to image profiling by linking images in the profile with related phrases in the event description.
          </p> 
        </div>
      </div>
    </li> -->

    </ul>

    <br><br><br>
  </div>

  <script src="js/jquery.min.js"></script>
  <script src="js/bootstrap.min.js"></script>
</body>
</html>